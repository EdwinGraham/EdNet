% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/EdNetTrain.R
\name{EdNetTrain}
\alias{EdNetTrain}
\title{Train a neural network model}
\usage{
EdNetTrain(
  X,
  Y,
  family=NULL,
  learning_rate=0.05,
  num_epochs,
  hidden_layer_dims=NULL,
  hidden_layer_activations=NULL,
  optimiser="GradientDescent",
  keep_prob=NULL,
  input_keep_prob=NULL,
  alpha=0,
  lambda=0,
  mini_batch_size=NULL,
  dev_set=NULL,
  beta1=ifelse(optimiser \%in\% c("Momentum", "Adam"), 0.9, NULL),
  beta2=ifelse(optimiser \%in\% c("RMSProp", "Adam"), 0.999, NULL),
  epsilon=ifelse(optimiser \%in\% c("RMSProp", "Adam"), 1E-8, NULL),
  print_every_n=100L,
  seed=1984L,
  plot=TRUE,
  checkpoint=NULL,
  keep=FALSE
)
}
\arguments{
\item{X}{A matrix with rows as training examples and columns as input features}

\item{Y}{A matrix with rows as training examples and columns as target values}

\item{family}{Type of regression to be performed. One of "binary", "multiclass", "gaussian", "poisson", "gamma", "tweedie".
Will be ignored if starting from a checkpoint model. Alternatively you can specify a named list with the following elements:
"family" - a character of length 1 for reference only (must use "multiclass" if target values have dimension > 1);
"link.inv" - a function (the inverse link function for activating the output layer);
"costfun" - a function with parameters 'Y'and 'Y_hat' representing the cost function to be minimised;
"gradfun" - a function with parameters 'Y'and 'Y_hat' representing the gradient of the cost function with respect to the linear, pre-activation, matrix in the output layer.}

\item{learning_rate}{Learning rate to use.}

\item{num_epochs}{Number of epochs (complete pass through training data) to be performed.
If using mini-batches the number of iterations may be much higher.}

\item{hidden_layer_dims}{Integer vector representing the dimensions of the hidden layers.
Should not be specified if starting from a checkpoint model.}

\item{hidden_layer_activations}{Character vector the same length as the \code{hidden_layer_dims} vector or length 1.
If length is 1 the same activation function will be used for all hidden layers.
Should only contain "relu" or "tanh" as these are the only supported activation functions for hidden layers.
Should not be specified if starting from a checkpoint model.}

\item{optimiser}{Type of optimiser to use. One of "GradientDescent", "Momentum", "RMSProp", "Adam".}

\item{keep_prob}{Keep probabilities for applying drop-out in hidden layers.
Either a constant or a vector the same length as the \code{hidden_layer_dims} vector. If NULL no drop-out is applied.}

\item{input_keep_prob}{Keep probabilities for applying drop-out in the input layer.
Needs to be a single constant. If NULL no drop-out is applied.}

\item{alpha}{L1 regularisation term.}

\item{lambda}{L2 regularisation term.}

\item{mini_batch_size}{Size of mini-batches to use. If NULL full training set is used for each iteration.}

\item{dev_set}{Integer vector representing hold-out data. Integers refer to individual training examples in the order presented in X.}

\item{beta1}{Exponential weighting term for gradients when using Momentum or Adam optimisation.}

\item{beta2}{Exponential weighting term for square pf gradients when using RMSProp or Adam optimisation.}

\item{epsilon}{Small number used for numerical stability to prevent division by zero when using RMSProp or Adam optimisation.}

\item{print_every_n}{Print info to the log every n epochs. If NULL, no printing is done.}

\item{seed}{Random seed to use for repeatability.}

\item{plot}{Plot cost function when printing to log.}

\item{checkpoint}{Rather than initialise new parameters, start from a checkpoint model.}

\item{keep}{keep X and Y data in final output.}
}
\value{
An object of class EdNetModel.
}
\description{
Train a neural network model
}
\examples{
# No example yet
}
\author{
Edwin Graham <edwingraham1984@gmail.com>
}
