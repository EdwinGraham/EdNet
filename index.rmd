---
title: "EdNet Vignette"
author: Edwin Graham <edwingraham1984@gmail.com>
output:
  html_document:
    toc: true
---

## Introduction

EdNet is a package for training deep neural network models with fully connected layers on a number of different regression and classification tasks. This package was built as a personal project to develop my coding skills and my understanding of neural networks and the algorithms used to train them. While there are many deep learning libraries out there that are faster and contain far greater functionality, this one is simple and easy to use in R. You may find it useful if you want a neural network package in R to play around with in order to improve your understanding and hone your intuition on network architectures and hyperparameters.

Currently EdNet supports binary and multiclass classification as well as regression for the following distributions: Normal, Poisson, Gamma, Tweedie. Drop-out and L1 and L2 regularisation are also supported, as well as mini-batch learning.

## Installation

EdNet can be installed directly from GitHub using the `devtools` package. The code is below.

```{r eval=FALSE, include=TRUE}
devtools::install_github("EdwinGraham/EdNet")
```

## Binary classification example

Load in some dummy binary classification data and view.

```{r}
library(EdNet)
data("dfBinary")
head(dfBinary)
```

There are 3 random normally distributed variables, x1, x2 and x3 and a binary target variable that depends in some probabilistic way on the other variables. We will train a neural network with 5 hidden layers for 10 epochs, using out the first 200 rows as holdout data.

```{r}
modelFit <- EdNetTrain(X=as.matrix(dfBinary[, 1:3]),
                       Y=as.matrix(dfBinary[, 4]),
                       family="binary",
                       learning_rate=0.01,
                       num_epochs=40,
                       hidden_layer_dims=c(64, 64, 32, 32, 16),
                       hidden_layer_activations="relu",
                       optimiser="Adam",
                       mini_batch_size=32,
                       print_every_n=40L,
                       dev_set = 1:200)
```

There may be able to acheive better results by tuning the various parameters, for example adding drop-out or regularisation. Now we have build a model, We can use the generic `predict` function to score probabilities onto the dataset.

```{r}
dfBinary$prediction <- predict(modelFit, as.matrix(dfBinary[, 1:3]))
head(dfBinary)
```

Looking only at the rows we held out, we can calculate an accuracy metric.

```{r}

devSetAccuracy <- mean((dfBinary$prediction[1:200]>.5)==dfBinary$target[1:200])
devSetAccuracy
```

The model we've built acheives a 87% accuracy on the hold-out data, so we are doing a lot better than random.

## Multiclass classification with Iris data

EdNet also supports multiclass classification. The following example uses the famous Iris dataset to train a NN to predict iris species using sepal and petal lengths and widths.

First we bring the Iris data into the environment.

```{r}
data("iris")
head(iris)

# Plot sepal length v sepal width and colour by species
plot(iris$Sepal.Length, iris$Sepal.Width, col=iris$Species, pch=16,
     xlab="Sepal Length", ylab="Sepal Width", main="Iris species by sepal length and width")
legend("topright",
       legend=c("setosa", "versicolor", "virginica"),
       col=1:3, pch=16)

```

Before fitting the model. For multi-class learning, `EdNetTrain` requires the input labels to be one-hot encoded. It's also good practice to normalise input features before training.

```{r}
X <- sapply(iris[, 1:4], normalise)
head(X)

Y <- onehotEncode(iris$Species)
head(Y)
```

We will hold out 20 rows at random (there are 150 altogether) in order to test the model is working as well as we expect.

```{r}
set.seed(1984)
holdoutRows <- sample(1:150, 20)
```

We're now ready to train a model. We will train a network with 3 fully-connected hidden layers with dimensions 128, 64 and 32. Each of these layers will use a `relu` activiation function and a drop-out percentage of 50%. We will train for 40 epochs with a mini-batch size of 16 using Adam optimisation.

```{r}
modelFit <- EdNetTrain(X, Y,
                       family="multiclass",
                       learning_rate=0.01,
                       num_epochs=40,
                       hidden_layer_dims=c(128, 64, 32),
                       hidden_layer_activations="relu",
                       optimiser="Adam",
                       lambda = 0.001,
                       keep_prob=0.5,
                       mini_batch_size=16,
                       print_every_n=40L,
                       dev_set=holdoutRows)
```

The costs are calculated on the final mini-batch of an epoch (in this case only on 16 rows for the training data) which, along with the drop-out, explains the noisiness. It seems quite flat after a certain point and the goodness-of-fit on the dev set is not trending upwards so we can be reasonably happy with the number of epochs we trained for.

We can generate the predicted class probabilities by using the `predict` function.


```{r}
classProbs <- predict(modelFit, X)
head(classProbs)
```

Using the `predictedClass` function we can calculate the predicted class for each row (which is the class with the highest probability.)

```{r}
iris$predictions <- predictedClass(classProbs, levels(iris$Species))
head(iris)
```

Plotting these predictions on the holdout only (it is easy to generate good predictions on the training data.)
```{r}

dev <- iris[holdoutRows, ]

par(mfrow=c(1,2))
plot(dev$Sepal.Length, dev$Sepal.Width, col=dev$Species, pch=16,
     xlab="Sepal Length", ylab="Sepal Width", main="Actual")
plot(dev$Sepal.Length, dev$Sepal.Width, col=dev$predictions, pch=16,
     xlab="Sepal Length", ylab="Sepal Width", main="Predicted")
legend("topright", inset=0.07,
       legend=c("setosa", "versicolor", "virginica"),
       col=1:3, pch=16)
```


Predictions on the holdout data set have 100% accuracy!

## Poisson regression example with offset model

Load in the dummy Poisson data.

```{r}
data("dfPoisson")
head(dfPoisson)
```

The data represents the results of a Poisson process where the `exposure` variable is the lenght of time the process was running for. 

```{r}
plot(dfPoisson$exposure, dfPoisson$target)
```

The target variable should depend linearly on exposure as well as on the other variables in some way. We can use an offset to take this into account.

```{r}
modelFit <- EdNetTrain(X=as.matrix(dfPoisson[, 1:3]),
                       Y=as.matrix(dfPoisson[, 5]),
                       offset=log(as.matrix(dfPoisson[, 4])),
                       family="poisson",
                       learning_rate=0.01,
                       num_epochs=40,
                       hidden_layer_dims=c(128, 32, 8),
                       hidden_layer_activations="relu",
                       optimiser="Adam",
                       lambda = 0.004,
                       keep_prob=0.8,
                       mini_batch_size=8,
                       print_every_n=40L,
                       dev_set=1:200)
```

Having built a model we can use `predict` to put predicted rate per exposure onto the original data.


```{r}
dfPoisson$predictedRate <- predict(modelFit, as.matrix(dfPoisson[, 1:3]))
dfPoisson$actualRate <- dfPoisson$target/dfPoisson$exposure
head(dfPoisson)
```

Plotting the actual v predicted rate on the hold-out data.

```{r}
plot(dfPoisson$predictedRate[1:200], dfPoisson$actualRate[1:200])
abline(c(0, 0), 1)
```

## Gamma distribution example and use of checkpoint model

Bring in some Gamma-distributed dummy data.

```{r}
data("dfGamma")
head(dfGamma)
```

Train a model for only 1 epoch.

```{r}
modelFit <- EdNetTrain(X=as.matrix(dfPoisson[, 1:3]),
                       Y=as.matrix(dfPoisson[, 4]),
                       family="gamma",
                       learning_rate=0.005,
                       num_epochs=1,
                       hidden_layer_dims=c(128, 32, 8),
                       hidden_layer_activations="relu",
                       optimiser="Adam",
                       lambda = 0.004,
                       keep_prob=0.8,
                       mini_batch_size=8,
                       print_every_n=1,
                       dev_set=1:200)
```

## Tweedie distribution example with weights

Example under construction
